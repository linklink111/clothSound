# Cloth Sound
Granular based cloth sound simulation.
## 2023年8月26日
今天改了clothSound的代码，解决了越往后合成的音频段越长的问题（step中t_curr 每次都多加了很多偏移t）。

目前基本实现了音轨编码速度比之前快很多，声音较为相似（在没有用滤波器的情况下），但是布料掉在球体上之后继续下落的过程似乎有很多的噪声，音频整体上也有很多噪声，以及较难设定一个目标长度时间来生成音频。

目前质点振动用的还是正弦波，计划改成点声源的波动方程（Howe等人）。

现在的声音辐射是根据每帧的加速度（由于m=1，等同于force）辐射一个频率固定的正弦波，或许可以根据不同的force辐射不同频率的波，例如Spring force，damping等各辐射出不同频率的波

如果写论文的话，可以写的创新点包括：
1. merge_t
2. 基于粒子的
3. 快速的（接近实时的）
4. 可以捕获关键的碰撞状态的
5. 分段编码时对于声音传播时间大于dt问题的处理

## 2023年8月28日
今天改了声音大小不统一的问题，在声压公式下方除以了一些与空气有关的系数，让声压数值更合理，也更便于归一化。
接下来要解决的是声音分段拼接之后不连续的问题。

## 2023年8月29日

尝试解决声音分段不连续的问题，打印dt和r/c发现r/c比dt大很多，得出结论：每个分段内声音逐渐衰减的原因是滞后的声音被单独编码导致的。
尝试采用重叠编码的方式，不可行。

现在分析miPhysics的音频处理部分。
初步分析，感觉要用多线程。

进一步了解了miPhysics，实在是......太强了。光从他们官网上的文档就能学到很多东西。（但是不知道为什么他们的论文之前都没看到过（可能是因为他们比较偏工程？））

在zlib下载了一些书

## 2023年8月30日

感觉今天还是要注意解决一下声音不连续的问题

先实现个多线程？
等等，既然能多线程了，有没有可能用GPU处理音频也是可行的。（本质上就是数组排序和插值吧？）

突然想起来昨天在miPhysics文档上看到的一句话很有启发性：In waveguide methods, we decompose physical motion into travelling waves in opposite directions, linked together by scattering junctions
这似乎和我模拟布料声音的思路有一点相似，不知道travelling waves和它的link是怎么实现的。

在考虑了后续可能还要实现复杂的控制功能之后，我又向blender求助了（救命）
在前面的研究中我已经验证了将单个质点作为发声单元的可行性（miPhysics的质点声学系统的叫法也印证了这一点（由于我不知道它具体是什么，所以只能通过名称来考量一下hh））
由于质点的独立性，不像之前的纱线一样难以建模，为复杂动画或结构的模拟做了准备。

拆解出一个关键的小问题：通过布料的point cache估计每个时刻质点的加速度（1.直接算斜率；2.神经网络？）

感觉这周还是先出点效果比较好

为了导出blender中的模拟数据稍微研究了一下point cache。看了一圈觉得这篇比较有用https://code.blender.org/2011/01/a-look-at-point-cache/（这里记下来防止以后要用）
原来mdd文件是light weight point cache的意思，之前用过但是不知道。

写了一个类似于数据处理的工作流：（因为菜所以不能实时处理数据只能像数据分析一样处理orz）
1. 读取mdd
2. 差分出速度和加速度
3. 应用诺伊曼边界条件把加速度的变化率转为压强
4. 根据t排序，插值p，写为wav

 长远计划：
 1. 模拟布料的声音（如果有优化的话或许可以多发几篇文章）(差分法求加速度发一篇，神经网络求加速度再发一篇，或许还可以用神经网络把加速度转换成声压？先用神经网络学习对布料动画的分类（摩擦、碰撞...一切从分类开始）......)
 2. 模拟软体的声音（软体声音目前也只有数据驱动的方法。与布料类似，如果有优化的话或许可以多发几篇文章）
 3. 模拟地面的声音
 4. 毕业论文写非线性物体的声音模拟，把之前做的都写进来。

